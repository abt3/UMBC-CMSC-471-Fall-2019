{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 471: Introduction to Artificial Intelligence</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Chapter 4 - Local Search<br>Hill Climbing, Simulated Annealing,  <br>Gradient Descent & Genetic Algorithms</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b> Announcements, updates and reminders</b> \n",
    "- <b> Chapter 4: Local Search and Optimization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- <b>Announcements, updates and reminders:</b>\n",
    "    - Quiz 4 release date: Tue Sep 24, Due Tue Oct 1.\n",
    "    - Quiz 5 Release Date: Tue Oct 1. Due Tue Oct 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Assignment 1 \"Uninformed Search\"](https://nbviewer.jupyter.org/github/fereydoonvafaei/UMBC-CMSC-471-Fall-2019/blob/master/Assignment-1/Assignment-1.0.ipynb) Due: Thu Sep 26 11:59PM.\n",
    "\n",
    "- Assignment 2 - Release date: Fri Sep 27 11:00PM, Due Wed Oct 9 11:59PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    - Schedule and reading assignments for this week (Sep 23-29):\n",
    "    Chapter 5 \"Adversarial Search\".\n",
    "    - Schedule and reading assignments for next week (Sep 30 - Oct 6):\n",
    "    Chapter 6 \"Constraint Satisfaction Problems\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Chapter 4: Beyond Classical Search</center></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Local Search</center></h1>\n",
    "\n",
    "<h5><center>Methods Inspired by Statistical Physics (Simulated Annealing) and Evolutionary Biology (Genetic Algorithms)</center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Local Search</center></h1>\n",
    "\n",
    "- Local search algorithms operate using a single current node (rather than multiple paths), and generally move only to neighbors of that node.\n",
    "\n",
    "- Local serach algorithms are useful for solving <font color=\"blue\">optimization problems</font>, in which the aim is to find the best state according to an <font color=\"blue\">objective function</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Optimization Problems</center></h1>\n",
    "\n",
    "<img src=\"img/global-maximum.png\" align=\"center\"/>\n",
    "\n",
    "From Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "- Also known as Steepest Ascent: This is our first example of a local search algorithm.\n",
    "\n",
    "\n",
    "- Imagine you are climbing a mountain and you are in a very thick fog. You can only see a distance equal to one step length. To try to climb you take the step in the direction that is steepest to get to the highest point of all the locations you can currently see.\n",
    "\n",
    "\n",
    "- In other words, hill-climbing search simply evaluates the objective function for all states that are neighbors to the current state, and takes the neighbor state with the best objective function value as the new current state. If there are more than one next best states, one is picked randomly.\n",
    "\n",
    "\n",
    "- Hill-climbing search is sometimes called greedy search, because a step is taken after only considering the immediate neighbors. No time is spent considering possible future states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "<img src=\"img/sa-1.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "<img src=\"img/hc.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing Failure</center></h1>\n",
    "\n",
    "<img src=\"img/hc-fail.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hill Climbing</center></h1>\n",
    "\n",
    "Hill-climbing is easy to formulate and implement and often finds\n",
    "pretty good states quickly.  But, it has the following problems:\n",
    "\n",
    "  * it gets stuck on local optima (hills for maximizing searches, valleys for minimizing searches,\n",
    "  * it may get stuck on a ridge, if no single action can advance the search along the ridge,\n",
    "  * it may get stuck wandering on a plateau for which all neighboring states have equal value.\n",
    "\n",
    "Common variations include\n",
    "  * allow sideways moves (when on a plateau)\n",
    "  * stochastic hill-climbing: choose next state with probability related to increase in value of objective function\n",
    "  * first-choice hill-climbing: generate neighbors by random choice of available actions and keep first state that has better value,\n",
    "  * random-restart hill climbing: conduct multiple hill-climbing searches from multiple, randomly generated, initial states.\n",
    "\n",
    "Only this last one, with random-restarts, is **complete**.  In the limit, all states will be tried as starting states so the goal, or best state, will eventually be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Simulated Annealing</center></h1>\n",
    "\n",
    "- Hill-climbing searches will get stuck on local optima.  Only by adding random restarts can you have a hill-climbing algorithm that is complete.\n",
    "\n",
    "\n",
    "- To get off of a local optimum, a search must be defined to allow steps that are \"downhill\" for maximizing searches, and \"uphill\" for minimizing searches, away from the optimum.  \n",
    "\n",
    "\n",
    "- [**Simulated annealing**](https://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf) is an algorithm that does this probabilisitically.  In metallurgy, **annealing** is the process used to temper or harden metals and glass by heating them to a high temprature and then gradually cooling them, thus allowing the material to reach a low-energy crystalline state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Assume we are doing a maximizing search, meaning we want to find the state with the maximum value.  Let the value of the current state be $v$.  Imagine an action has been applied to that state and the resulting state has a lower (worse) value $v'$.  Simulated annealing will accept this new state as the current state with probability: $$e^{(v' - v)/T}$$\n",
    "\n",
    "- $T$ is like a \"temperature\", the higher the value the more likely we are to take a step to a state with a worse value. In practice, $T$ starts at a high value and is slowly decreased towards zero.  If it is decreased \"slowly enough\", the global optimum will be found with probabilty 1.  In other words, this is a **complete** algorithm if the cooling strategy is slow enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Simulated Annealing</center></h1>\n",
    "\n",
    "<img src=\"img/sa-1.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Simulated Annealing - Ping-Pong Ball Shaking Intuition</center></h1>\n",
    "\n",
    "<img src=\"img/sa-ping-pong.jpg\" align=\"center\"/>\n",
    "\n",
    "Image from: https://rs.io/ultimate-guide-simulated-annealing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Descent</center></h1>\n",
    "\n",
    "<img src=\"img/gradient.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Hands-On ML Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Descent - Learning Rate Too Small</center></h1>\n",
    "\n",
    "<img src=\"img/learning-rate-1.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Hands-On ML Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Descent - Learning Rate Too Large</center></h1>\n",
    "\n",
    "<img src=\"img/learning-rate-2.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Hands-On ML Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Optimization Demos](https://www.deeplearning.ai/ai-notes/optimization/?utm_source=social&utm_medium=linkedin&utm_campaign=BlogAINotesOptimizationAugust272019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>8-Queens Problem</center></h1>\n",
    "\n",
    "<img src=\"img/8queen.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Genetic Algorithm - 8 Queens</center></h1>\n",
    "\n",
    "<img src=\"img/genetic-8queen.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Genetic Algorithm - Inspired by Evolution Theory</center></h1>\n",
    "\n",
    "<img src=\"img/genetic.png\" align=\"center\"/>\n",
    "\n",
    "Image from: Russel & Norvig Textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Genetic Algorithm Demo -1](http://math.hws.edu/eck/js/genetic-algorithm/GA.html)\n",
    "\n",
    "[Geneti Algorithm Demo -2](https://www.youtube.com/watch?v=XcinBPhgT7M)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
